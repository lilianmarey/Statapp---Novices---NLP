{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Préparation des données\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modèles\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Option d'affchage\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la base des données calculées\n",
    "\n",
    "df0 = pd.read_csv('data/bdd_complete2.csv', sep = ',', encoding = 'latin-1')\n",
    "\n",
    "# Copie de la base\n",
    "\n",
    "#df = df0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    "       'Date', 'Nature.de.séance',\n",
    "       'Réplique', 'Didascalie',\n",
    "       'sexe', 'age',\n",
    "       'groupe.sigle', 'commissions', 'nb.mandats', 'cabcollab', 'duree.pol',\n",
    "       'clustRFSP', 'clustVEP', 'hautdip', 'majo',\n",
    "       'profsigni2', 'Groupe'\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df.apply(lambda row: (datetime.strptime(row.Date, '%Y-%m-%d') - datetime(2015, 1, 1, 0, 0)).days, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Didascalie'] = df.apply(lambda row: 0 if str(row.Didascalie) == 'nan' else 1, axis =  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sexe'] = df.apply(lambda row: 0 if row.sexe == 'F' else 1, axis =  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['majo'] = df.apply(lambda row: 1 if row.majo else 0, axis =  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Groupe'] = df.apply(lambda row: 1 if row.Groupe == 'Exp' else 0, axis =  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cabcollab'] = df.apply(lambda row: 1 if row.cabcollab else 0, axis =  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation des données\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df[['Date', 'age'\n",
    "    ]] = min_max_scaler.fit_transform(df[['Date', 'age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(dataframe, variable):\n",
    "    \n",
    "    dataframe[variable] = dataframe[variable].astype(str)\n",
    "\n",
    "\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    var = variable + '_encoded'\n",
    "    \n",
    "    dataframe[var] = le.fit_transform(dataframe[variable])\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "\n",
    "    X = ohe.fit_transform(dataframe[var].values.reshape(-1,1)).toarray()\n",
    "\n",
    "    dfOneHot = pd.DataFrame(X, columns = [variable + str(int(i)) for i in range(X.shape[1])])\n",
    "    dataframe = pd.concat([dataframe, dfOneHot], axis=1).drop(columns = [variable, var]).dropna()\n",
    "    \n",
    "    return dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = one_hot_encode(df, 'Nature.de.séance')\n",
    "df = one_hot_encode(df, 'groupe.sigle')\n",
    "df = one_hot_encode(df, 'commissions')\n",
    "df = one_hot_encode(df, 'clustRFSP')\n",
    "df = one_hot_encode(df, 'clustVEP')\n",
    "df = one_hot_encode(df, 'hautdip')\n",
    "df = one_hot_encode(df, 'profsigni2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('data/one_hot_encoded_data.csv')\n",
    "df = pd.read_csv('data/one_hot_encoded_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[i for i in df.columns if not(i[:3] in ['nom', 'Nat', 'Pré'])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit la variable objectif\n",
    "\n",
    "df['Réplique'] = df.apply(lambda row : len(row.Réplique), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['Réplique']]\n",
    "X = df.drop(columns = ['Réplique'])\n",
    "\n",
    "# On choisit un échantillon de validation de 20 %\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la matrice de corrélation\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (13, 10)) \n",
    "fig = sns.heatmap(df.sample(100).corr(), cmap= 'coolwarm', annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1 Regressions linéaires et polynomiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression linéaire multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par une regression linéaire pour avoir une première idée de l'influence des variables sur le prix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# On prédit à partir de l'échantillon de test pour calculer les scores\n",
    "\n",
    "y_pred = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "px.histogram(pd.DataFrame([\n",
    "    (X.columns[i], lin_reg.coef_[0][i]) for i in range(47)\n",
    "                    ]).T.rename(index = {0 : 'variable', 1 : 'coeff'}).T, x = 'variable', y = 'coeff', histfunc = 'sum'\n",
    "            ).show()\n",
    "\n",
    "print('MSE :', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des p-values\n",
    "\n",
    "mod = sm.OLS(y,X)\n",
    "fii = mod.fit()\n",
    "p_values = fii.summary2().tables[1]['P>|t|']\n",
    "pd.DataFrame(p_values).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que :  \n",
    "\n",
    "- les variables avec les plus petites p-values sont les score aux commerces et score_monument\n",
    "- être proche des commerces atypiques mais loin des commerces en général ferait monter le prix\n",
    "- la variable qui semble être la plus significative est score_monument (p-value la plus basse sans compter les TypeVoie, et coefficient le plus grand)\n",
    "- score_metro a étonnement un coefficient négatif\n",
    "- Les p-values concernant TypeVoie ne semblent pas pertinentes, on manque sans doute de données pour certaines modalités"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressions polynomiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est très probable que les variables n'aient pas un effet linéaire sur le prix au m<sup>2<sup>.\n",
    "    \n",
    "C'est pourquoi on choisit de faire des regressions polynomiales, en augmentant progressivement le degré, et en s'arrêtant dès qu'on observe de l'overfitting sur l'échantillon de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_reg(n):\n",
    "    \"\"\"\n",
    "    Cette fonction effectue une regression polynomiale sur la base d'entrainement et affiche \n",
    "    la MSE pour l'echantillon de test\n",
    "    \"\"\"\n",
    "    \n",
    "    # Agrandissement de la base d'entrainement avec l'ajout des degrés successifs\n",
    "    poly_X = X_train.copy()\n",
    "    \n",
    "    for i in range(2, n+1):\n",
    "        \n",
    "        index = [str(j) + '^' + str(i) for j in X.columns[:32]]\n",
    "        poly_X[[str(j) + '^' + str(i) for j in X.columns[:32]]] = poly_X[[j for j in X.columns[:32]]].pow(i)\n",
    "    \n",
    "    poly_reg = LinearRegression().fit(poly_X, y_train)\n",
    "    \n",
    "    # Agrandissement de la base d'de test avec l'ajout des degrés successifs\n",
    "    poly_X_test = X_test.copy()\n",
    "    \n",
    "    for i in range(2, n+1):\n",
    "        \n",
    "        index = [str(j) + '^' + str(i) for j in X.columns[:32]]\n",
    "        poly_X_test[[str(j) + '^' + str(i) for j in X.columns[:32]]] = poly_X_test[[j for j in X.columns[:32]]].pow(i)\n",
    "    \n",
    "    # Regression\n",
    "    y_pred = poly_reg.predict(poly_X_test)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Affichage des coefficients pour le degré 2\n",
    "    fig = px.histogram(pd.DataFrame([\n",
    "        (poly_X.columns[i], poly_reg.coef_[0][i]) for i in range(28 + 32 * (n - 1))\n",
    "                            ]).T.rename(\n",
    "        index = {0 : 'variable', 1 : 'coeff'}\n",
    "                                        ).T.sort_values(by = 'variable'), x = 'variable', y = 'coeff') \n",
    "    \n",
    "    if n == 2:\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print('degré : ', n)\n",
    "\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Regressions jusqu'à l'overfitting (MSE > 0.4 sur l'échantillon test)\n",
    "\n",
    "i = 2\n",
    "val = poly_reg(2)\n",
    "list_mse_degree = [val]\n",
    "\n",
    "\n",
    "while val < .016:\n",
    "    i += 1\n",
    "    val = poly_reg(i)\n",
    "    list_mse_degree.append(val) \n",
    "    \n",
    "    if i > 25:\n",
    "        break\n",
    "    \n",
    "\n",
    "px.line(x = [i + 2 for i in range(len(list_mse_degree))], y = list_mse_degree).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que :  \n",
    "- le degré à partir duquel on observe de l'overfitting dépend beaucoup de l'échantillon de test, qui est choisi au hasard. On a pu observer de l'overfitting au degré 25 comme au degré 8 avec des échantillons différents\n",
    "- Dans la regression de degré 2, pour presque chaque variable, le coefficient de la variable et de la variable au carré ont un exposant de signe opposé. Cela semble signifier que le premier coeffient \"compense\" l'effet du deuxième, et donc que l'effet de chaque variable est plus complexe qu'un effet linéaire\n",
    "- Seul score_monument a significativement deux coefficients positif. Son effet semble clair : plus on est proche des monuments, plus le prix est haut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour obtenir de nouveaux échantillons si échec :  \n",
    "y = df[['prixm2']]\n",
    "X = df.drop(columns = ['prixm2'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2 Regression pénalisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etant donnée le grand nombre de variables dont nous disposons, il nous parrait cohérent d'utiliser une regression pénalisée afin de sélectionner les variables les plus significatives.\n",
    "De plus, certaines de nos variables sont corrélées entre elles.\n",
    "\n",
    "Ici, on a fait baisser l1_ratio progressivement jusqu'à obtenir 10 variables dont les coefficients sont non nul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "\n",
    "y = df1[['Réplique']]\n",
    "X = df1.drop(columns = ['Réplique'])\n",
    "\n",
    "# On choisit un échantillon de validation de 20 %\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_reg = ElasticNet(alpha=.3, copy_X=False, fit_intercept = False, l1_ratio=1)\n",
    "\n",
    "EN_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = EN_reg.predict(X_test)\n",
    "print('MSE : ', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([(X.columns[i], EN_reg.coef_[i]) for i in range(N)]).T.rename(index = {0 : 'variable', 1 : 'coeff'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [(EN_reg.coef_[i], X.columns[i]) for i in range(N) if abs(EN_reg.coef_[i]) > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.3 Regression par Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons faire des regressions grâce aux arbres de décision. \n",
    "Nous allons rechercher les meilleurs paramètres pour notre regression.\n",
    "Pour cela, on teste un à un les paramètres suivants :  \n",
    "- profondeur des arbres  \n",
    "- nombre d'arbres  \n",
    "- minimun d'exemples requis pour splitter l'arbre  \n",
    "- nombre de feuilles maximum par arbre  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit d'abord une fonction qui donne l'importance des variables vis à vis d'un certain modèle\n",
    "\n",
    "def feat_importance(model, x_train, y_train, X):\n",
    "    \"\"\"\n",
    "    Renvoie le tableau de l'importance des variables vis à vis du modèle par la méthode des permutations\n",
    "    \"\"\"\n",
    "\n",
    "    result = permutation_importance(\n",
    "                                    model, \n",
    "                                    X, \n",
    "                                    y, \n",
    "                                    n_repeats = 3,\n",
    "                                    random_state = 0\n",
    "                                    )['importances_mean']\n",
    "    \n",
    "    importance = pd.DataFrame(result, index = X.columns, columns = [\"Importance\"])\n",
    "    \n",
    "    return importance.sort_values(by = ['Importance'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des essembles de tests et d'entrainement, on choisit une taile de test de 30% ici\n",
    "\n",
    "X = df.drop(['Réplique', 'Unnamed: 0'], axis = 1)\n",
    "x = np.array(X)\n",
    "y = np.array(df['Réplique'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 41)\n",
    "# random_state correspond à la graine générant l'échantillon aléatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3.1 Profondeur maximale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_depth_ls = [1, 10, 13, 15, 17, 20, 25, 30] # profondeurs maximales des arbres de décision testées\n",
    "mse_train_max_depth = []\n",
    "mse_test_max_depth = []\n",
    "\n",
    "# Pour chaque profondeur max, on regresse avec random forest\n",
    "\n",
    "for m in max_depth_ls :\n",
    "    \n",
    "    print('Profondeur téstée : ', m)\n",
    "    \n",
    "    rf = RandomForestRegressor(\n",
    "                            max_depth = m, \n",
    "                            random_state=0,\n",
    "                            n_estimators = 30) # nombre d'arbres utilisés\n",
    "    \n",
    "    rf = rf.fit(x_train, y_train)\n",
    "    y_pred_train = rf.predict(x_train)\n",
    "    y_pred = rf.predict(x_test)\n",
    "    \n",
    "    mse_train_max_depth.append(mean_squared_error(y_train, y_pred_train))\n",
    "    mse_test_max_depth.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche ensuite les performances de la regression sur les deux échantillon (train et test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(max_depth_ls, mse_train_max_depth, color = 'red', label = 'Train')\n",
    "plt.plot(max_depth_ls, mse_test_max_depth, color = 'blue', label = 'Test')\n",
    "plt.title('MSE en fonction de max_depth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que l'overfitting débute après max_depth > 15 (le score sur l'échantillon d'entrainement continue de decroitre alors qu'il commence à croitre sur l'échantillon de test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On regarde la valeur qui minimise la MSE sur l'ensemble de test\n",
    "\n",
    "max_depth_ls[mse_test_max_depth.index(min(mse_test_max_depth))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On garde donc max_depth = 15 pour la suite**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3.2 Nombre d'arbres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde maintenant l'effet du nombre d'arbre sur l'effet de la regression\n",
    "\n",
    "nb_estimators_ls = [1, 2, 3, 5, 20, 40, 50, 60, 80]\n",
    "mse_train_nb_estimators = []\n",
    "mse_test_nb_estimators = []\n",
    "\n",
    "for m in nb_estimators_ls :\n",
    "    print(\"Nombre d'arbres testés : \", m)\n",
    "    rf = RandomForestRegressor(max_depth = 15, \n",
    "                               random_state = 0,\n",
    "                                n_estimators = m)    \n",
    "    \n",
    "    rf = rf.fit(x_train, y_train)\n",
    "    y_pred_train = rf.predict(x_train)\n",
    "    y_pred = rf.predict(x_test)\n",
    "    \n",
    "    mse_train_nb_estimators.append(mean_squared_error(y_train, y_pred_train))\n",
    "    mse_test_nb_estimators.append(mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche ensuite les performances de la regression sur les deux échantillon (train et test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(nb_estimators_ls, mse_train_nb_estimators, color = 'red', label = 'Train')\n",
    "plt.plot(nb_estimators_ls, mse_test_nb_estimators, color = 'blue', label = 'Test')\n",
    "plt.title('MSE en fonction de n_estimators')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La courbe semble strictement décroissante, on doit arbitrer entre complexité algorithmique et performance.  \n",
    "On garde n_estimators = 60 pour la suite**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3.3 Minimun d'exemples requis pour splitter l'arbre  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fait varier le nombre minimum d'exemple requis pour créer une feuille/noeud\n",
    "\n",
    "samples_leaf_ls = [1, 2, 3, 4, 10]\n",
    "mse_train_samples_leaf = []\n",
    "mse_test_samples_leaf = []\n",
    "\n",
    "\n",
    "for m in samples_leaf_ls :\n",
    "    print('min_samples_leaf testé : ', m)\n",
    "    rf = RandomForestRegressor( max_depth = 15, \n",
    "                                min_samples_leaf = m,\n",
    "                                n_estimators = 60, \n",
    "                                random_state = 0\n",
    "                              )    \n",
    "    \n",
    "    rf = rf.fit(x_train, y_train)\n",
    "    y_pred_train = rf.predict(x_train)\n",
    "    y_pred = rf.predict(x_test)\n",
    "    \n",
    "    mse_train_samples_leaf.append(mean_squared_error(y_train, y_pred_train))\n",
    "    mse_test_samples_leaf.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche ensuite les performances de la regression sur les deux échantillon (train et test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(samples_leaf_ls, mse_train_samples_leaf, color='red', label='Train')\n",
    "plt.plot(samples_leaf_ls, mse_test_samples_leaf, color='blue', label='Test')\n",
    "plt.title('MSE en fct de min samples leaf')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On garde donc min_samples_leaf = 1 pour la suite**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3.4 Nombre maximum de feuilles par arbres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_leaf_ls = [2, 10, 100, 150, 200, 1000, 1500]\n",
    "mse_train_max_leaf = []\n",
    "mse_test_max_leaf = []\n",
    "\n",
    "\n",
    "\n",
    "for m in max_leaf_ls :\n",
    "    \n",
    "    print('Nombre de feuilles max testé : ', m)\n",
    "    rf = RandomForestRegressor(max_depth = 15, \n",
    "                               min_samples_leaf = 1, \n",
    "                               max_leaf_nodes = m,\n",
    "                               n_estimators = 60)   \n",
    "    \n",
    "    rf = rf.fit(x_train, y_train)\n",
    "    y_pred_train = rf.predict(x_train)\n",
    "    y_pred = rf.predict(x_test)\n",
    "    mse_train_max_leaf.append(mean_squared_error(y_train, y_pred_train))\n",
    "    mse_test_max_leaf.append(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche ensuite les performances de la regression sur les deux échantillon (train et test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(max_leaf_ls, mse_train_max_leaf, color = 'red', label = 'Train')\n",
    "plt.plot(max_leaf_ls, mse_test_max_leaf, color = 'blue', label = 'Test')\n",
    "plt.title('MSE en fonction max_leaf_nodes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La courbe semble strictement décroissante, on doit arbitrer entre complexité algorithmique et performance.  \n",
    "On garde max_leaf_nodes = 1000 pour la suite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a maintenant tous nos paramètres\n",
    "\n",
    "print('a')\n",
    "rf = RandomForestRegressor(\n",
    "                        max_depth = 7, \n",
    "                        min_samples_leaf = 1, \n",
    "                        max_leaf_nodes = 1000,\n",
    "                        n_estimators = 20\n",
    "                            )    \n",
    "print('b')\n",
    "\n",
    "\n",
    "rf = rf.fit(x_train, y_train)\n",
    "print('c')\n",
    "\n",
    "\n",
    "y_pred_train = rf.predict(x_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "oo = np.zeros(y_pred.shape)\n",
    "\n",
    "print('MSE train : ', mean_squared_error(y_train, y_pred_train))\n",
    "print('MSE test : ', mean_squared_error(y_test, y_pred))\n",
    "print('MSE modèle nulle : ', mean_squared_error(y_test, oo))\n",
    "\n",
    "importance = feat_importance(rf, x_train, y_train, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance.plot(kind = 'barh', figsize = (18, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rappel des variables retenue par ElasticNet : ', signif_EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On retrouve la présence de score_monument comme variable la plus importante, et on retrouve globalement les mêmes variables importantes.  \n",
    "- On retrouve que la longitude est plus importante que la latitude.  \n",
    "- On note aussi que les TypeVoie les plus importantes sont les 1, 2 et 11 (Avenue, Boulevard, Rue). \n",
    "- Par contre, Quartier surpasse Arrondissement de beaucoup, comme ces deux variables sont très corrélées, cela ne pose pas de problème de cohérence.\n",
    "- NbPieces, et les scores pour les jardins, le metro et les commerce sont plus iportants ici.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On garde que les variables raisonnablement significatives (>0.01)\n",
    "\n",
    "X = df[['score_monument', 'lon', 'lat', 'NbPieces', 'score_jardin', 'score_metro']]\n",
    "\n",
    "x = np.array(X)\n",
    "y = np.array(df['prixm2'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 41)\n",
    "\n",
    "rf = RandomForestRegressor(max_depth = 15, random_state = 0, min_samples_leaf = 1, max_leaf_nodes = 1000,\n",
    "                                 n_estimators = 60)\n",
    "rf = rf.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "y_pred_train= rf.predict(x_train)\n",
    "\n",
    "oo = np.zeros(y_pred.shape)\n",
    "\n",
    "print('MSE train : ', mean_squared_error(y_train, y_pred_train))\n",
    "print('MSE test : ', mean_squared_error(y_test, y_pred))\n",
    "print('MSE model nulle : ', mean_squared_error(y_test, oo))\n",
    "\n",
    "importance = feat_importance(rf, x_train, x_test, X)\n",
    "importance.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retrouve exactement les mêmes résultats, sauf que la latitude a dépassé la longitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4 Réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les réseaux de neurones sont des architectures qui permettent de rendre compte des influences complexes des variables sur le prix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit les échantillons\n",
    "\n",
    "y = df[['prixm2']]\n",
    "X = df.drop(columns = ['prixm2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après plusieurs essais, un réseau 28 - 2 - 3 - 1 fournit des résultats satisfaisant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction de l'architecture du réseau\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(70, input_dim = 47, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(12))\n",
    "model.add(Dense(13))\n",
    "model.add(Dense(15))\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lancement de la phase d'apprentissage\n",
    "\n",
    "history = model.fit(X, y, validation_split = 0.2,  epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche l'évolution de la loss au fil des époques pour les échantillons train et test.\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Evolution de MSE sur X_train au fil des époques')\n",
    "plt.ylabel('mse')\n",
    "plt.legend(['train'], loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Evolution de MSE sur X_test au fil des époques')\n",
    "plt.ylabel('mse')\n",
    "plt.legend(['test'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **En prenant en compte les variables sélectionnées par ElsaticNet précédemment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_significatif = X[signif_EN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On reprend la même architecture en changeant le nombre de neurones d'entrée.\n",
    "\n",
    "model_significatif = Sequential()\n",
    "\n",
    "model_significatif.add(Dense(2, input_dim = 10, activation='relu'))\n",
    "model_significatif.add(Dense(3))\n",
    "model_significatif.add(Dense(1))\n",
    "\n",
    "model_significatif.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lancement de la phase d'apprentissage\n",
    "\n",
    "history_significatif = model_significatif.fit(X_significatif, y, validation_split = 0.2,  epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche l'évolution de la loss au fil des époques pour les échantillons \n",
    "# train et test en comparant avec l'autre réseau.\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(np.array(history_significatif.history['loss']))\n",
    "plt.title('Evolution de MSE sur X_train au fil des époques')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel('mse')\n",
    "plt.legend(['train significatif', 'train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.plot(history_significatif.history['val_loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Evolution de MSE sur X_test au fil des époques')\n",
    "plt.ylabel('mse')\n",
    "plt.legend(['test_significatif', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :  \n",
    "- le comportement de la loss dépend de la sélection de l'échantillon de test. On a pu avoir des modèles qui apprenaient bien (décroissance de la loss dans les 4 graphiques précedents), comme des modèles qui apprenaient mal.\n",
    "- La plupart du temps, on observe quand même un apprentissage qui fonctionne. Mais les performance des deux réseaux sont après 15 époques similaires (l'apprentissage avec plus de variable est même plus efficace avec peu d'époques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphiques obtenus dans une bonne situation\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "img = mpimg.imread('img/img0.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "img = mpimg.imread('img/img1.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance des paramètres (méthode des permutations)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(model, scoring = 'r2', random_state = 1).fit(X,y)\n",
    "eli5.show_weights(perm, feature_names = X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm, feature_names = X.columns.tolist(), top = 47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate ici en général l'importance du score_monument, largement les plus important selon ce modèle (poids 10 fois supérieur aux autres variables).\n",
    "On retrouve aussi l'importance du Quartier et de la longitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous voulons classer les appartements en différents cluster, afin d'identifier des appartements \"types\", regroupant les mêmes caractéristiques classiques.  \n",
    "Nous procédons alors à une ACP pour réduire la dimension de nos données, puis nous utilisons la méthode des k moyennes pour le clustering. \n",
    "On pondère chaque variable par son importance donnée dans le réseau de neurones précedemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ACP = df[signif_EN].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ACP['Arrondissement'] *= 0.0045\n",
    "df_ACP['Quartier'] *= 0.0167\n",
    "df_ACP['score_monument'] *= 0.0555\n",
    "df_ACP['TypeVoie_1'] *= 0.0030\n",
    "df_ACP['lat'] *= 0.0003\n",
    "df_ACP['lon'] *= 0.0087\n",
    "df_ACP['score_commerce_lux'] *= 0.0004\n",
    "df_ACP['TypeVoie_11'] *= 0.0003\n",
    "df_ACP['periode_construction'] *= 0.0004\n",
    "df_ACP['TypeVoie_2'] *= 0.0008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On réduit sur 3 variables\n",
    "\n",
    "pca = PCA(n_components = 3)\n",
    "pca.fit(df_ACP)\n",
    "\n",
    "transformed_df = pd.DataFrame(pca.transform(df_ACP)).rename(columns = {\n",
    "                            0 : 'var0', \n",
    "                            1 : 'var1', \n",
    "                            2 : 'var2',\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(transformed_df.sample(1000), x = 'var0', y = 'var1', z = 'var2', opacity = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut retrouver dans cette représentation une \"hélice\", qui fait penser aux arrondissements de Paris disposés en escargot. Cela suggère une forte composante géographique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On distinguera alors 6 clusters.\n",
    "\n",
    "kmeans = KMeans(n_clusters = 6).fit(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On labélise\n",
    "\n",
    "transformed_df['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(transformed_df.sample(1000), x = 'var0', y = 'var1', z = 'var2', color = 'cluster', opacity = .7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les clusters ont bien été faits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On affiche ensuite les différentes caractéristiques des clusters obtenus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster0 = df_ACP.iloc[[i for i in list(transformed_df[transformed_df['cluster'] == 0].index)]]\n",
    "df_cluster1 = df_ACP.iloc[[i for i in list(transformed_df[transformed_df['cluster'] == 1].index)]]\n",
    "df_cluster2 = df_ACP.iloc[[i for i in list(transformed_df[transformed_df['cluster'] == 2].index)]]\n",
    "df_cluster3 = df_ACP.iloc[[i for i in list(transformed_df[transformed_df['cluster'] == 3].index)]]\n",
    "df_cluster4 = df_ACP.iloc[[i for i in list(transformed_df[transformed_df['cluster'] == 4].index)]]\n",
    "df_cluster5 = df_ACP.iloc[[i for i in list(transformed_df[transformed_df['cluster'] == 5].index)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cluster0.describe().T[['mean', 'std']].T)\n",
    "display(df_cluster1.describe().T[['mean', 'std']].T)\n",
    "display(df_cluster2.describe().T[['mean', 'std']].T)\n",
    "display(df_cluster3.describe().T[['mean', 'std']].T)\n",
    "display(df_cluster4.describe().T[['mean', 'std']].T)\n",
    "display(df_cluster5.describe().T[['mean', 'std']].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche le résultat dans un plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ACP['cluster'] = pd.Series(kmeans.labels_)\n",
    "df_ACP[['true_lat', 'true_lon']] = df0[['lat', 'lon']]\n",
    "\n",
    "px.scatter(df_ACP, x = 'true_lon', y = 'true_lat', color = 'cluster', template = 'none', opacity = .3, color_continuous_scale = 'Jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a en effet un clustering très géographique. On peut essayer d'interpréter les clusters :   \n",
    "- Quartiers populaires (19ème, 20ème, une partie du 18ème et l'extrême sud)\n",
    "- Quartiers familiaux (12ème, 13ème, 14ème, 15ème)\n",
    "- Quartiers riche 1 (16ème)\n",
    "- Quartier riche 2 (7ème et 16ème)\n",
    "- Quartiers historiques (centre)\n",
    "- Quartiers qui s'embourgeoisent (autour des Quartiers historiques de Paris centre, 9ème, 10ème, 11ème)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient des modèles dont les performances sont représentées ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(x = ['Reg. Lin.', 'Reg. poly.', 'ElasticNet', 'Random Forest', 'Réseau de neurones'], \n",
    "             y = [0.014, 0.013, 0.018, 0.012, 0.012], \n",
    "             range_y = [.01,.018],\n",
    "             labels = {'x' : 'Modèle', 'y' : 'MSE'},\n",
    "             template = 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate la supériorité des réseaux de neurones et des regressions par Ramdom Forest, avec un avantage pour ces dernières car elles semblent plus robustes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au niveau de l'analyse des variables les plus importantes, on retrouve au fil des modèles :\n",
    "- une omniprésence de score_monument\n",
    "- une supériorité d'importance de lon sur lat\n",
    "- une grande importance du Quartier ou de l'Arrondissement\n",
    "- un effet positif du score_commerce_lux et négatif du score_commerce"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
