{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:02:18: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 21:02:18: built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "# Importation des modules\n",
    "\n",
    "import pandas as pd\n",
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#Affichage de toutes les colonnes\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('data/data_cleaned_NLP.csv', sep = ',', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df0[['Réplique', 'Groupe', 'tokenized_replique']].copy()\n",
    "\n",
    "df2 = df1[['Groupe', 'tokenized_replique']].copy()\n",
    "\n",
    "df_novice = df2[df2['Groupe'] == 'Novice'].copy()\n",
    "df_exp = df2[df2['Groupe'] == 'Exp'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_novice = df_novice.dropna()\n",
    "\n",
    "df_exp = df_exp.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_novice = [row.split() for row in df_novice['tokenized_replique']]\n",
    "\n",
    "sent_exp = [row.split() for row in df_exp['tokenized_replique']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 20:52:11: collecting all words and their counts\n",
      "INFO - 20:52:11: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 20:52:12: PROGRESS: at sentence #20000, processed 544399 words and 367942 word types\n",
      "INFO - 20:52:14: PROGRESS: at sentence #40000, processed 1061844 words and 645502 word types\n",
      "INFO - 20:52:14: collected 773338 word types from a corpus of 1326670 words (unigram + bigrams) and 50191 sentences\n",
      "INFO - 20:52:14: using 773338 counts as vocab in Phrases<0 vocab, min_count=3, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 20:52:14: collecting all words and their counts\n",
      "INFO - 20:52:14: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 20:52:15: PROGRESS: at sentence #20000, processed 518787 words and 365570 word types\n",
      "INFO - 20:52:17: PROGRESS: at sentence #40000, processed 1056472 words and 658662 word types\n",
      "INFO - 20:52:18: PROGRESS: at sentence #60000, processed 1530225 words and 884119 word types\n",
      "INFO - 20:52:19: PROGRESS: at sentence #80000, processed 2021109 words and 1099782 word types\n",
      "INFO - 20:52:20: PROGRESS: at sentence #100000, processed 2530754 words and 1316508 word types\n",
      "INFO - 20:52:22: PROGRESS: at sentence #120000, processed 3091195 words and 1542161 word types\n",
      "INFO - 20:52:22: PROGRESS: at sentence #140000, processed 3492363 words and 1690307 word types\n",
      "INFO - 20:52:23: PROGRESS: at sentence #160000, processed 3827516 words and 1796729 word types\n",
      "INFO - 20:52:24: PROGRESS: at sentence #180000, processed 4116140 words and 1885664 word types\n",
      "INFO - 20:52:25: PROGRESS: at sentence #200000, processed 4652876 words and 2075194 word types\n",
      "INFO - 20:52:26: PROGRESS: at sentence #220000, processed 5173769 words and 2259183 word types\n",
      "INFO - 20:52:27: PROGRESS: at sentence #240000, processed 5648678 words and 2425973 word types\n",
      "INFO - 20:52:28: PROGRESS: at sentence #260000, processed 6139387 words and 2575760 word types\n",
      "INFO - 20:52:29: PROGRESS: at sentence #280000, processed 6634584 words and 2729208 word types\n",
      "INFO - 20:52:31: PROGRESS: at sentence #300000, processed 7164382 words and 2893910 word types\n",
      "INFO - 20:52:32: PROGRESS: at sentence #320000, processed 7711168 words and 3057317 word types\n",
      "INFO - 20:52:33: PROGRESS: at sentence #340000, processed 8221718 words and 3206295 word types\n",
      "INFO - 20:52:34: PROGRESS: at sentence #360000, processed 8672434 words and 3335018 word types\n",
      "INFO - 20:52:35: PROGRESS: at sentence #380000, processed 8861978 words and 3371134 word types\n",
      "INFO - 20:52:36: PROGRESS: at sentence #400000, processed 9333965 words and 3508565 word types\n",
      "INFO - 20:52:37: PROGRESS: at sentence #420000, processed 9789642 words and 3631848 word types\n",
      "INFO - 20:52:38: PROGRESS: at sentence #440000, processed 10262034 words and 3757534 word types\n",
      "INFO - 20:52:39: collected 3813404 word types from a corpus of 10479107 words (unigram + bigrams) and 448965 sentences\n",
      "INFO - 20:52:39: using 3813404 counts as vocab in Phrases<0 vocab, min_count=3, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases_novice = Phrases(sent_novice, min_count = 3, progress_per = 20000)\n",
    "\n",
    "phrases_exp = Phrases(sent_exp, min_count = 3, progress_per = 20000)\n",
    "\n",
    "\n",
    "# min_count : Ignore all words and bigrams with total collected count lower than this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 20:52:39: source_vocab length 773338\n",
      "INFO - 20:52:49: Phraser built with 16435 phrasegrams\n",
      "INFO - 20:52:49: source_vocab length 3813404\n",
      "INFO - 20:52:59: Phraser added 50000 phrasegrams\n",
      "INFO - 20:53:37: Phraser built with 73757 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "bigram_novice = Phraser(phrases_novice)\n",
    "\n",
    "bigram_exp = Phraser(phrases_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_novice = bigram_novice[sent_novice]\n",
    "\n",
    "sentences_exp = bigram_exp[sent_exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_novice = defaultdict(int)\n",
    "word_freq_exp = defaultdict(int)\n",
    "\n",
    "for sent in sentences_novice:\n",
    "    for i in sent:\n",
    "        word_freq_novice[i] += 1\n",
    "        \n",
    "for sent in sentences_exp:\n",
    "    for i in sent:\n",
    "        word_freq_exp[i] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choix des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 00:14:18: collecting all words and their counts\n",
      "INFO - 00:14:18: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 00:14:19: PROGRESS: at sentence #10000, processed 233965 words, keeping 30761 word types\n",
      "INFO - 00:14:20: PROGRESS: at sentence #20000, processed 450032 words, keeping 39850 word types\n",
      "INFO - 00:14:21: PROGRESS: at sentence #30000, processed 677772 words, keeping 48410 word types\n",
      "INFO - 00:14:22: PROGRESS: at sentence #40000, processed 880381 words, keeping 51772 word types\n",
      "INFO - 00:14:22: PROGRESS: at sentence #50000, processed 1095690 words, keeping 55266 word types\n",
      "INFO - 00:14:22: collected 55316 word types from a corpus of 1099932 raw words and 50191 sentences\n",
      "INFO - 00:14:22: Loading a fresh vocabulary\n",
      "INFO - 00:14:25: effective_min_count=5 retains 26248 unique words (47% of original 55316, drops 29068)\n",
      "INFO - 00:14:25: effective_min_count=5 leaves 1041812 word corpus (94% of original 1099932, drops 58120)\n",
      "INFO - 00:14:25: deleting the raw counts dictionary of 55316 items\n",
      "INFO - 00:14:25: sample=6e-05 downsamples 1069 most-common words\n",
      "INFO - 00:14:25: downsampling leaves estimated 749202 word corpus (71.9% of prior 1041812)\n",
      "INFO - 00:14:25: estimated required memory for 26248 words and 300 dimensions: 76119200 bytes\n",
      "INFO - 00:14:25: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# Choix de la fenêtre, on aimerait que le modèle aprenne vite, on alongera ensuite le nombre d'epochs\n",
    "\n",
    "liste_modeles = [Word2Vec(\n",
    "                     window = i,\n",
    "                     size = 300,\n",
    "                     sample = 6e-5, \n",
    "                     alpha = 0.03, \n",
    "                     min_alpha = 0.0007, \n",
    "                     negative = 20,\n",
    "                     workers = cores - 1,\n",
    "                     compute_loss = True)\n",
    "                 \n",
    "                 for i in range(1, 10)\n",
    "                ]\n",
    "\n",
    "L = []\n",
    "\n",
    "for i in range(len(liste_modeles)):\n",
    "    \n",
    "    model = liste_modeles[i]\n",
    "    model.build_vocab(sentences_novice, progress_per = 10000)\n",
    "\n",
    "\n",
    "    model.train(sentences_novice, total_examples = w2v_model_novice.corpus_count, epochs = 5, report_delay = 1)\n",
    "\n",
    "    L.append([\n",
    "            (model.wv.most_similar(positive=['droite'])[i][0],\n",
    "             model.wv.most_similar(positive=['vitesse'])[i][0],\n",
    "             model.wv.most_similar(positive=['donc'])[i][0],\n",
    "            )\n",
    "         for i in range(10)\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['résolu',\n",
       "  'vécu',\n",
       "  'assemblée_sénat',\n",
       "  'représentés',\n",
       "  'abouti',\n",
       "  'voter_texte',\n",
       "  'débouché',\n",
       "  'fait_unanimité',\n",
       "  'france_comores',\n",
       "  'personnalisée'],\n",
       " ['entendue',\n",
       "  'veux_saluer',\n",
       "  'a_su',\n",
       "  'parvenus',\n",
       "  'connaissez_bien',\n",
       "  'banc',\n",
       "  'abouti',\n",
       "  'votées',\n",
       "  'juillet_dernier',\n",
       "  'voter_texte'],\n",
       " ['gauche',\n",
       "  'remercie_avoir',\n",
       "  'veux_saluer',\n",
       "  'statu_quo',\n",
       "  'seuls_contre',\n",
       "  'députés_majorité',\n",
       "  'parvenus',\n",
       "  'connaissez_bien',\n",
       "  'fermement',\n",
       "  'habitude'],\n",
       " ['gauche',\n",
       "  'confirmer',\n",
       "  'débat_parlementaire',\n",
       "  'tribune',\n",
       "  'habitude',\n",
       "  'socialistes',\n",
       "  'entendue',\n",
       "  'sincèrement',\n",
       "  'entendus',\n",
       "  'beaucoup_choses'],\n",
       " ['oppositions',\n",
       "  'gauche',\n",
       "  'socialistes',\n",
       "  'habitude',\n",
       "  'banc',\n",
       "  'députée',\n",
       "  'beaucoup_choses',\n",
       "  'exprimés',\n",
       "  'chère',\n",
       "  'commencer']]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_novice = Word2Vec(\n",
    "                     window = 1,\n",
    "                     size = 300,\n",
    "                     sample = 6e-5, \n",
    "                     alpha = 0.03, \n",
    "                     min_alpha = 0.0007, \n",
    "                     negative = 20,\n",
    "                     workers = cores - 1,\n",
    "                     compute_loss = True)\n",
    "\n",
    "w2v_model_exp = Word2Vec(\n",
    "                     window = 4,\n",
    "                     size = 300,\n",
    "                     sample = 6e-5, \n",
    "                     alpha = 0.03, \n",
    "                     min_alpha = 0.0007, \n",
    "                     negative = 20,\n",
    "                     workers = cores - 1,\n",
    "                     compute_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:28:19: collecting all words and their counts\n",
      "INFO - 21:28:19: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 21:28:19: PROGRESS: at sentence #10000, processed 233965 words, keeping 30761 word types\n",
      "INFO - 21:28:20: PROGRESS: at sentence #20000, processed 450032 words, keeping 39850 word types\n",
      "INFO - 21:28:21: PROGRESS: at sentence #30000, processed 677772 words, keeping 48410 word types\n",
      "INFO - 21:28:22: PROGRESS: at sentence #40000, processed 880381 words, keeping 51772 word types\n",
      "INFO - 21:28:22: PROGRESS: at sentence #50000, processed 1095690 words, keeping 55266 word types\n",
      "INFO - 21:28:22: collected 55316 word types from a corpus of 1099932 raw words and 50191 sentences\n",
      "INFO - 21:28:22: Loading a fresh vocabulary\n",
      "INFO - 21:28:22: effective_min_count=5 retains 26248 unique words (47% of original 55316, drops 29068)\n",
      "INFO - 21:28:22: effective_min_count=5 leaves 1041812 word corpus (94% of original 1099932, drops 58120)\n",
      "INFO - 21:28:23: deleting the raw counts dictionary of 55316 items\n",
      "INFO - 21:28:23: sample=6e-05 downsamples 1069 most-common words\n",
      "INFO - 21:28:23: downsampling leaves estimated 749202 word corpus (71.9% of prior 1041812)\n",
      "INFO - 21:28:23: estimated required memory for 26248 words and 300 dimensions: 76119200 bytes\n",
      "INFO - 21:28:23: resetting layer weights\n",
      "INFO - 21:28:29: training model with 3 workers on 26248 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.18 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:28:30: EPOCH 1 - PROGRESS: at 14.59% examples, 119085 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:28:31: EPOCH 1 - PROGRESS: at 29.88% examples, 113220 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:32: EPOCH 1 - PROGRESS: at 41.41% examples, 104014 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:33: EPOCH 1 - PROGRESS: at 55.41% examples, 104272 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:34: EPOCH 1 - PROGRESS: at 70.25% examples, 104299 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:35: EPOCH 1 - PROGRESS: at 85.67% examples, 104728 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:28:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:28:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:28:36: EPOCH - 1 : training on 1099932 raw words (749232 effective words) took 6.9s, 108082 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:28:37: EPOCH 2 - PROGRESS: at 14.59% examples, 123502 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:28:38: EPOCH 2 - PROGRESS: at 32.44% examples, 125605 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:39: EPOCH 2 - PROGRESS: at 50.17% examples, 128152 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:40: EPOCH 2 - PROGRESS: at 66.47% examples, 126086 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:41: EPOCH 2 - PROGRESS: at 82.48% examples, 121550 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:42: EPOCH 2 - PROGRESS: at 97.94% examples, 120822 words/s, in_qsize 2, out_qsize 1\n",
      "INFO - 21:28:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:28:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:28:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:28:42: EPOCH - 2 : training on 1099932 raw words (749796 effective words) took 6.2s, 121716 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:28:43: EPOCH 3 - PROGRESS: at 13.68% examples, 114145 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:44: EPOCH 3 - PROGRESS: at 31.64% examples, 120600 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:45: EPOCH 3 - PROGRESS: at 45.26% examples, 115764 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:46: EPOCH 3 - PROGRESS: at 60.80% examples, 114749 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:47: EPOCH 3 - PROGRESS: at 78.07% examples, 116350 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:28:48: EPOCH 3 - PROGRESS: at 92.94% examples, 114964 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:28:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:28:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:28:49: EPOCH - 3 : training on 1099932 raw words (749338 effective words) took 6.6s, 113274 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:28:50: EPOCH 4 - PROGRESS: at 11.29% examples, 88611 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:51: EPOCH 4 - PROGRESS: at 26.72% examples, 98172 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:52: EPOCH 4 - PROGRESS: at 42.19% examples, 105034 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:53: EPOCH 4 - PROGRESS: at 59.89% examples, 110851 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:54: EPOCH 4 - PROGRESS: at 76.24% examples, 111668 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:28:55: EPOCH 4 - PROGRESS: at 92.94% examples, 112536 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:28:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:28:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:28:56: EPOCH - 4 : training on 1099932 raw words (748841 effective words) took 6.6s, 113427 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:28:57: EPOCH 5 - PROGRESS: at 13.68% examples, 112279 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:58: EPOCH 5 - PROGRESS: at 31.64% examples, 117498 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:28:59: EPOCH 5 - PROGRESS: at 48.16% examples, 118506 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:00: EPOCH 5 - PROGRESS: at 64.46% examples, 118211 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:01: EPOCH 5 - PROGRESS: at 80.86% examples, 116209 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:02: EPOCH 5 - PROGRESS: at 97.79% examples, 117621 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 21:29:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:02: EPOCH - 5 : training on 1099932 raw words (749292 effective words) took 6.3s, 118681 effective words/s\n",
      "INFO - 21:29:02: training on a 5499660 raw words (3746499 effective words) took 32.7s, 114621 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 4: 0.0\n",
      "Time to train the model: 0.54 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model_novice.build_vocab(sentences_novice, progress_per = 10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model_novice.train(sentences_novice, total_examples = w2v_model_novice.corpus_count, epochs = 5, report_delay = 1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:29:02: collecting all words and their counts\n",
      "INFO - 21:29:02: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 21:29:03: PROGRESS: at sentence #10000, processed 233965 words, keeping 30761 word types\n",
      "INFO - 21:29:04: PROGRESS: at sentence #20000, processed 450032 words, keeping 39850 word types\n",
      "INFO - 21:29:05: PROGRESS: at sentence #30000, processed 677772 words, keeping 48410 word types\n",
      "INFO - 21:29:05: PROGRESS: at sentence #40000, processed 880381 words, keeping 51772 word types\n",
      "INFO - 21:29:06: PROGRESS: at sentence #50000, processed 1095690 words, keeping 55266 word types\n",
      "INFO - 21:29:06: collected 55316 word types from a corpus of 1099932 raw words and 50191 sentences\n",
      "INFO - 21:29:06: Loading a fresh vocabulary\n",
      "INFO - 21:29:06: effective_min_count=5 retains 26248 unique words (47% of original 55316, drops 29068)\n",
      "INFO - 21:29:06: effective_min_count=5 leaves 1041812 word corpus (94% of original 1099932, drops 58120)\n",
      "INFO - 21:29:06: deleting the raw counts dictionary of 55316 items\n",
      "INFO - 21:29:06: sample=6e-05 downsamples 1069 most-common words\n",
      "INFO - 21:29:06: downsampling leaves estimated 749202 word corpus (71.9% of prior 1041812)\n",
      "INFO - 21:29:06: estimated required memory for 26248 words and 300 dimensions: 76119200 bytes\n",
      "INFO - 21:29:06: resetting layer weights\n",
      "INFO - 21:29:13: training model with 3 workers on 26248 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.19 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:29:14: EPOCH 1 - PROGRESS: at 8.94% examples, 75749 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:15: EPOCH 1 - PROGRESS: at 18.67% examples, 74386 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:16: EPOCH 1 - PROGRESS: at 29.08% examples, 72742 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:17: EPOCH 1 - PROGRESS: at 42.19% examples, 79422 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:18: EPOCH 1 - PROGRESS: at 55.41% examples, 82884 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:19: EPOCH 1 - PROGRESS: at 69.26% examples, 85337 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:20: EPOCH 1 - PROGRESS: at 83.31% examples, 86118 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:21: EPOCH 1 - PROGRESS: at 96.69% examples, 87870 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:22: EPOCH - 1 : training on 1099932 raw words (749440 effective words) took 8.5s, 88145 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:29:23: EPOCH 2 - PROGRESS: at 12.09% examples, 102900 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:24: EPOCH 2 - PROGRESS: at 28.29% examples, 108079 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:25: EPOCH 2 - PROGRESS: at 42.96% examples, 108831 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:26: EPOCH 2 - PROGRESS: at 57.44% examples, 108245 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:27: EPOCH 2 - PROGRESS: at 73.50% examples, 109556 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:28: EPOCH 2 - PROGRESS: at 89.34% examples, 110488 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:28: EPOCH - 2 : training on 1099932 raw words (749026 effective words) took 6.7s, 111941 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:29:29: EPOCH 3 - PROGRESS: at 13.68% examples, 116468 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:30: EPOCH 3 - PROGRESS: at 30.72% examples, 118834 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:31: EPOCH 3 - PROGRESS: at 45.26% examples, 116017 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:32: EPOCH 3 - PROGRESS: at 61.71% examples, 117252 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:33: EPOCH 3 - PROGRESS: at 76.24% examples, 112925 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:34: EPOCH 3 - PROGRESS: at 84.88% examples, 103613 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:35: EPOCH 3 - PROGRESS: at 97.79% examples, 102387 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:36: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:36: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:36: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:36: EPOCH - 3 : training on 1099932 raw words (749041 effective words) took 7.4s, 100947 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:29:37: EPOCH 4 - PROGRESS: at 12.09% examples, 96841 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:38: EPOCH 4 - PROGRESS: at 28.29% examples, 104410 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:39: EPOCH 4 - PROGRESS: at 42.96% examples, 107451 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:40: EPOCH 4 - PROGRESS: at 58.21% examples, 109042 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:41: EPOCH 4 - PROGRESS: at 74.28% examples, 110096 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:42: EPOCH 4 - PROGRESS: at 89.34% examples, 109838 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:43: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:43: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:43: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:43: EPOCH - 4 : training on 1099932 raw words (749485 effective words) took 7.0s, 107413 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:29:44: EPOCH 5 - PROGRESS: at 5.60% examples, 45192 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:45: EPOCH 5 - PROGRESS: at 18.67% examples, 72414 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:46: EPOCH 5 - PROGRESS: at 32.44% examples, 80824 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:47: EPOCH 5 - PROGRESS: at 49.26% examples, 91395 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:48: EPOCH 5 - PROGRESS: at 66.47% examples, 97581 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:49: EPOCH 5 - PROGRESS: at 84.93% examples, 102339 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:50: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:50: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:50: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:50: EPOCH - 5 : training on 1099932 raw words (749059 effective words) took 7.1s, 105619 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 4: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:29:51: EPOCH 6 - PROGRESS: at 13.68% examples, 107661 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:29:52: EPOCH 6 - PROGRESS: at 28.29% examples, 104059 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:53: EPOCH 6 - PROGRESS: at 43.78% examples, 109781 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:54: EPOCH 6 - PROGRESS: at 59.89% examples, 112487 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:55: EPOCH 6 - PROGRESS: at 77.09% examples, 114525 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:56: EPOCH 6 - PROGRESS: at 94.60% examples, 116083 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:29:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:29:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:29:56: EPOCH - 6 : training on 1099932 raw words (748910 effective words) took 6.4s, 117051 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 5: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:29:57: EPOCH 7 - PROGRESS: at 14.59% examples, 116907 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:29:58: EPOCH 7 - PROGRESS: at 32.44% examples, 121593 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:29:59: EPOCH 7 - PROGRESS: at 46.07% examples, 115467 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:00: EPOCH 7 - PROGRESS: at 61.71% examples, 115066 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:01: EPOCH 7 - PROGRESS: at 80.86% examples, 116855 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:02: EPOCH 7 - PROGRESS: at 97.79% examples, 118155 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:03: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:03: EPOCH - 7 : training on 1099932 raw words (749052 effective words) took 6.3s, 118804 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 6: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:30:04: EPOCH 8 - PROGRESS: at 14.59% examples, 115425 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:30:05: EPOCH 8 - PROGRESS: at 32.44% examples, 120776 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:06: EPOCH 8 - PROGRESS: at 47.15% examples, 117979 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:07: EPOCH 8 - PROGRESS: at 63.54% examples, 117762 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:08: EPOCH 8 - PROGRESS: at 79.48% examples, 114819 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:09: EPOCH 8 - PROGRESS: at 95.64% examples, 115588 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:09: EPOCH - 8 : training on 1099932 raw words (749594 effective words) took 6.4s, 116618 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 7: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:30:10: EPOCH 9 - PROGRESS: at 14.59% examples, 117248 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:11: EPOCH 9 - PROGRESS: at 32.44% examples, 120240 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:12: EPOCH 9 - PROGRESS: at 49.26% examples, 122064 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:13: EPOCH 9 - PROGRESS: at 65.34% examples, 121538 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:14: EPOCH 9 - PROGRESS: at 83.31% examples, 121394 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:15: EPOCH 9 - PROGRESS: at 97.79% examples, 118012 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:15: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:15: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:15: EPOCH - 9 : training on 1099932 raw words (749475 effective words) took 6.4s, 117322 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 8: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:30:16: EPOCH 10 - PROGRESS: at 13.68% examples, 116357 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:17: EPOCH 10 - PROGRESS: at 30.72% examples, 119219 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:18: EPOCH 10 - PROGRESS: at 47.15% examples, 121133 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:30:19: EPOCH 10 - PROGRESS: at 64.46% examples, 121811 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:20: EPOCH 10 - PROGRESS: at 83.31% examples, 122310 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:30:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:30:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:30:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:30:21: EPOCH - 10 : training on 1099932 raw words (749640 effective words) took 6.1s, 123889 effective words/s\n",
      "INFO - 21:30:21: training on a 10999320 raw words (7492722 effective words) took 68.4s, 109599 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 9: 0.0\n",
      "Time to train the model: 1.14 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model_exp.build_vocab(sentences_novice, progress_per = 10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model_exp.train(sentences_novice, total_examples = w2v_model_exp.corpus_count, epochs = 10, report_delay = 1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:07:23: precomputing L2-norms of word weight vectors\n",
      "INFO - 23:07:23: precomputing L2-norms of word weight vectors\n",
      "INFO - 23:07:23: saving Word2Vec object under results/word2vec_novice.model, separately None\n",
      "INFO - 23:07:23: not storing attribute vectors_norm\n",
      "INFO - 23:07:23: not storing attribute cum_table\n",
      "INFO - 23:07:23: saved results/word2vec_novice.model\n",
      "INFO - 23:07:23: saving Word2Vec object under results/word2vec_exp.model, separately None\n",
      "INFO - 23:07:23: not storing attribute vectors_norm\n",
      "INFO - 23:07:23: not storing attribute cum_table\n",
      "INFO - 23:07:23: saved results/word2vec_exp.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model_novice.init_sims(replace = True)\n",
    "w2v_model_exp.init_sims(replace = True)\n",
    "\n",
    "w2v_model_novice.save(\"results/word2vec_novice_test.model\")\n",
    "w2v_model_exp.save(\"results/word2vec_exp_test.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model.wv.most_similar(positive=[\"macron\"])\n",
    "#w2v_model.wv.most_similar(negative=[\"promesse\"])\n",
    "#w2v_model.wv.similarity(\"élection\", 'présidentielle')\n",
    "#w2v_model.wv.similarity(\"sport\", 'études')\n",
    "#print(w2v_model.wv.similarity(\"macron\", 'droite'))\n",
    "#print(w2v_model.wv.similarity(\"macron\", 'gauche'))\n",
    "#w2v_model.wv.doesnt_match(['gauche', 'président', 'droite'])\n",
    "#w2v_model.wv.most_similar(positive=[\"père\", \"femme\"], negative = ['homme'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gauche', 0.986516535282135),\n",
       " ('républicains', 0.9116498231887817),\n",
       " ('bancs', 0.9007534384727478),\n",
       " ('cet_hémicycle', 0.8813148736953735),\n",
       " ('hémicycle', 0.8689683675765991),\n",
       " ('france_insoumise', 0.8554179668426514),\n",
       " ('opposition', 0.8358474969863892),\n",
       " ('groupes', 0.802750825881958),\n",
       " ('voix', 0.800241231918335),\n",
       " ('groupe', 0.7991034984588623)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_novice.wv.most_similar(positive=[\"droite\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('droite_gauche', 0.6979137659072876),\n",
       " ('extrême_gauche', 0.6682906150817871),\n",
       " ('gauche', 0.6396193504333496),\n",
       " ('socialistes', 0.6157995462417603),\n",
       " ('majorité', 0.561996340751648),\n",
       " ('bancs', 0.5594807267189026),\n",
       " ('communistes', 0.5519882440567017),\n",
       " ('extrême_droite', 0.5514141321182251),\n",
       " ('rangs', 0.5431515574455261),\n",
       " ('oreille', 0.5347355008125305)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_exp.wv.most_similar(positive=[\"droite\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export_novice = pd.DataFrame(w2v_model_novice.wv.vectors)\n",
    "\n",
    "df_export_exp = pd.DataFrame(w2v_model_exp.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_novice = [w2v_model_novice.wv.most_similar(positive=[np.array(df_export_novice.iloc[i])])[0][0] for i in range(df_export_novice.shape[0])]\n",
    "\n",
    "words_exp = [w2v_model_exp.wv.most_similar(positive=[np.array(df_export_exp.iloc[i])])[0][0] for i in range(df_export_exp.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export_novice['word'] = words_novice\n",
    "\n",
    "df_export_exp['word'] = words_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export_novice.to_csv('results/embeddings_novice_test.csv')\n",
    "df_export_exp.to_csv('results/embeddings_exp_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
